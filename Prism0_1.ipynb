{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IerulXpZu-wo",
        "outputId": "c96b3b15-9f42-4088-c0db-7fd7d14df17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,Bidirectional,LSTM,GRU,Dense\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "nltk.download('punkt')\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('/content/train.txt','r')\n",
        "x_train=[]\n",
        "y_train=[]\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_train.append(l[1].strip())\n",
        "    x_train.append(l[0])\n",
        "f=open('/content/test.txt','r')\n",
        "x_test=[]\n",
        "y_test=[]\n",
        "x_real_test = []\n",
        "y_real_test = []\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_real_test.append(l[1].strip())\n",
        "    x_real_test.append(l[0])\n",
        "f=open('/content/val.txt','r')\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_test.append(l[1].strip())\n",
        "    x_test.append(l[0])\n",
        "data_train=pd.DataFrame({'Text':x_train,'Emotion':y_train})\n",
        "data_test=pd.DataFrame({'Text':x_test,'Emotion':y_test})\n",
        "data_real_test= pd.DataFrame({'Text':x_real_test,'Emotion':y_real_test})\n",
        "data_train.append(data_real_test,ignore_index=True)\n",
        "data=data_train.append(data_test,ignore_index=True)"
      ],
      "metadata": {
        "id": "sB0nzzv-vM3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(data):\n",
        "    data=re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
        "    data=re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
        "    data=word_tokenize(data)\n",
        "    return data\n",
        "texts=[' '.join(clean_text(text)) for text in data.Text]\n",
        "texts_train=[' '.join(clean_text(text)) for text in x_train]\n",
        "texts_test=[' '.join(clean_text(text)) for text in x_test]\n",
        "texts_real_test=[' '.join(clean_text(text)) for text in x_real_test]"
      ],
      "metadata": {
        "id": "0VQVlalnykQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequence_train=tokenizer.texts_to_sequences(texts_train)\n",
        "sequence_test=tokenizer.texts_to_sequences(texts_test)\n",
        "sequence_real_test=tokenizer.texts_to_sequences(texts_real_test)\n",
        "\n",
        "index_of_words=tokenizer.word_index\n",
        "vocab_size=len(index_of_words)+1"
      ],
      "metadata": {
        "id": "5yf8ICl4yold"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_real_test"
      ],
      "metadata": {
        "id": "yk5pIFikoc0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=6\n",
        "embed_num_dims=300\n",
        "max_seq_len=500\n",
        "class_names=['anger','sadness','fear','joy','surprise','love']\n",
        "X_train_pad=pad_sequences(sequence_train,maxlen=max_seq_len)\n",
        "X_test_pad=pad_sequences(sequence_test,maxlen=max_seq_len)\n",
        "X_real_test_pad=pad_sequences(sequence_real_test,maxlen=max_seq_len)\n",
        "\n",
        "encoding={'anger':0,'sadness':1,'fear':2,'joy':3,'surprise':4,'love':5}\n",
        "y_train=[encoding[x] for x in data_train.Emotion]\n",
        "y_test=[encoding[x] for x in data_test.Emotion]\n",
        "y_real_test=[encoding[x] for x in data_real_test.Emotion]\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)\n",
        "y_real_test=to_categorical(y_real_test)"
      ],
      "metadata": {
        "id": "MTDj5iH-ytxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(filepath,word_index,embedding_dim):\n",
        "    vocab_size=len(word_index)+1\n",
        "    embedding_matrix=np.zeros((vocab_size,embedding_dim))\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word,*vector=line.split()\n",
        "            if word in word_index:\n",
        "                idx=word_index[word]\n",
        "                embedding_matrix[idx] = np.array(vector,dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix\n",
        "fname='/content/drive/MyDrive/prism/wiki-news-300d-1M.vec'\n",
        "embedd_matrix=create_embedding_matrix(fname,index_of_words,embed_num_dims)"
      ],
      "metadata": {
        "id": "fRPD7Qjfyxhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myRunsZAmytx",
        "outputId": "6718809d-fe69-45c1-9f23-b24467250a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedd_layer=Embedding(vocab_size,embed_num_dims,input_length=max_seq_len,weights=[embedd_matrix],trainable=False)\n",
        "gru_output_size=128\n",
        "bidirectional=True\n",
        "model=Sequential()\n",
        "model.add(embedd_layer)\n",
        "model.add(Bidirectional(GRU(units=gru_output_size,dropout=0.2,recurrent_dropout=0.2)))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9R5jYghh5CzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=128\n",
        "epochs=4\n",
        "hist=model.fit(X_train_pad,y_train,batch_size=batch_size,epochs=epochs,validation_data=(X_test_pad,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbSrTvUq5Gkq",
        "outputId": "58686e9e-9a6a-4cbd-bacb-39f8293df89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "125/125 [==============================] - 1262s 10s/step - loss: 1.3941 - accuracy: 0.4825 - val_loss: 1.1446 - val_accuracy: 0.5725\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 1264s 10s/step - loss: 0.8743 - accuracy: 0.7009 - val_loss: 0.6241 - val_accuracy: 0.7815\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 1254s 10s/step - loss: 0.5207 - accuracy: 0.8176 - val_loss: 0.3721 - val_accuracy: 0.8755\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 1257s 10s/step - loss: 0.3413 - accuracy: 0.8802 - val_loss: 0.2520 - val_accuracy: 0.9140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"fin_model.h5\")"
      ],
      "metadata": {
        "id": "I8vYAsOiE1Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "#message=[emoji.demojize('😨😰😱').replace(\":\",\" \")]\n",
        "message=[\"im sad\"]\n",
        "seq=tokenizer.texts_to_sequences(message)\n",
        "padded=pad_sequences(seq,maxlen=max_seq_len)\n",
        "pred=model.predict(padded)\n",
        "#poda = model.evaluate(X_real_test_pad,y_real_test, verbose=0)\n",
        "print('Message:'+str(message))\n",
        "print('Emotion:',class_names[np.argmax(pred)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2ZqnnSH5NS_",
        "outputId": "c62dacc0-f15f-44e2-a404-f3a977e86b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 206ms/step\n",
            "Message:['im happy']\n",
            "Emotion: joy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.models.save_model(model,'textmodel',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImWTbl6x5SSx",
        "outputId": "0e6d46c1-ce97-44b0-cc6f-6a4be75a094d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3nTF7ZTsRlq",
        "outputId": "c4598cb8-733d-46b5-9b07-4f59014bf1b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=6081d91b9685425bd8b5a613a9dfa0a71e0b9cde93e66ac20bac7c77994da82b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = tf.keras.models.load_model(\"/content/fin_model.h5\")\n",
        "# load=load_model(\"/content/fin_model.h5\")\n",
        "message = [\"i am sad\"]\n",
        "seq = tokenizer.texts_to_sequences(message)\n",
        "padded = pad_sequences(seq, maxlen=500)\n",
        "pred = models.predict(padded)\n",
        "print(class_names[np.argmax(pred)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvAYQ0htezPX",
        "outputId": "d6c1cfff-243c-4790-f263-471c1d23566e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 750ms/step\n",
            "sadness\n"
          ]
        }
      ]
    }
  ]
}